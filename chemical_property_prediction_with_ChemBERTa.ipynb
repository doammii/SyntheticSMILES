{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4b49ff",
   "metadata": {},
   "source": [
    "### Install packages & libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb3e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e814099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.30.2\n",
    "!pip install simpletransformers\n",
    "!pip install datasets\n",
    "!pip install wandb\n",
    "!pip install matplotlib\n",
    "!pip install torch==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87551c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!test -d bertviz_repo && echo \"FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo\"\n",
    "!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
    "if not 'bertviz_repo' in sys.path:\n",
    "  sys.path += ['bertviz_repo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb35193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# !git clone https://github.com/seyonechithrananda/bert-loves-chemistry.git\n",
    "sys.path.append(\"./bert-loves-chemistry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06520c09",
   "metadata": {},
   "source": [
    "### Experiment settings & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1caaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUID_VISIBLE_DEVICES\"] = \"4\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import wandb\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import deepchem\n",
    "deepchem.__version__\n",
    "\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837735a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded - Splitting method: random\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_PATH = '' # Path to the dataset\n",
    "MODEL_FOLDER = '' # Path to save the model\n",
    "WANDB_API_KEY = 'YOUR_WANDB_API_KEY'  # Replace with your actual WandB API key\n",
    "\n",
    "# Splitting method configuration\n",
    "SPLITTING_METHOD = \"random\"  # Options: \"random\", \"stratified_kfold\", \"scaffold\"\n",
    "N_SPLITS = 5  # For K-fold cross-validation\n",
    "TEST_SIZE = 0.2\n",
    "VALID_SIZE = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# Model hyperparameters\n",
    "model_name = 'ChemBERTa-molecular-classification'\n",
    "project_name = 'molecular-classification'\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "learning_rate = 2e-5\n",
    "optimizer = 'AdamW'\n",
    "patience = 3\n",
    "manual_seed = SEED\n",
    "\n",
    "print(f\"Configuration loaded - Splitting method: {SPLITTING_METHOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "if WANDB_API_KEY:\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "else:\n",
    "    print(\"Warning: WANDB_API_KEY not provided. Wandb logging may not work.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a246e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, labels):\n",
    "    \"\"\"\n",
    "    Compute classification metrics including sensitivity and specificity\n",
    "    \"\"\"\n",
    "    # preds: (n_samples, n_classes) -> probability scores\n",
    "    # labels: (n_samples,) -> true labels\n",
    "    \n",
    "    pred_labels = preds.argmax(axis=1)\n",
    "    cm = confusion_matrix(labels, pred_labels)\n",
    "    \n",
    "    # Handle binary classification\n",
    "    if cm.shape == (2, 2):\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            \"TN\": int(TN),\n",
    "            \"FP\": int(FP),\n",
    "            \"FN\": int(FN),\n",
    "            \"TP\": int(TP),\n",
    "            \"sensitivity\": float(sensitivity),\n",
    "            \"specificity\": float(specificity),\n",
    "        }\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        accuracy = accuracy_score(labels, pred_labels)\n",
    "        metrics = {\n",
    "            \"accuracy\": float(accuracy)\n",
    "        }\n",
    "    \n",
    "    # Log metrics to wandb if available\n",
    "    try:\n",
    "        wandb.log(metrics)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8573ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_kwargs = {'name': model_name}\n",
    "\n",
    "classification_args = {\n",
    "    'evaluate_each_epoch': True,\n",
    "    'evaluate_during_training_verbose': True,\n",
    "    'evaluate_during_training': True,\n",
    "    'best_model_dir': MODEL_FOLDER,\n",
    "    'no_save': False,\n",
    "    'save_eval_checkpoints': False,\n",
    "    'save_model_every_epoch': False,\n",
    "    'save_best_model': True,\n",
    "    'save_steps': -1,\n",
    "    'save_limit': 1,\n",
    "    'num_train_epochs': EPOCHS,\n",
    "    'use_early_stopping': True,\n",
    "    'early_stopping_patience': patience,\n",
    "    'early_stopping_delta': 0.001,\n",
    "    'early_stopping_metric': 'eval_loss',\n",
    "    'early_stopping_metric_minimize': True,\n",
    "    'early_stopping_consider_epochs': True,\n",
    "    'fp16': False,\n",
    "    'optimizer': optimizer,\n",
    "    'adam_betas': (0.95, 0.999),\n",
    "    'learning_rate': learning_rate,\n",
    "    'manual_seed': manual_seed,\n",
    "    'train_batch_size': BATCH_SIZE,\n",
    "    'eval_batch_size': BATCH_SIZE,\n",
    "    'auto_weights': True,\n",
    "    'wandb_project': project_name,\n",
    "    'wandb_kwargs': wandb_kwargs,\n",
    "    'compute_metrics': compute_metrics,\n",
    "    'use_multiprocessing' : False,\n",
    "    'use_multiprocessing_for_evaluation': False,\n",
    "    'save_model_every_epoch': False,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Model configuration completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d79e095",
   "metadata": {},
   "source": [
    "### Dataset loading & splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9179031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Loaded CSV with shape: {df.shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Sanity check\n",
    "assert \"SMILES\" in df.columns and \"label\" in df.columns, \"Expected columns: SMILES and label\"\n",
    "\n",
    "# Remove rows with missing SMILES or labels\n",
    "df = df.dropna(subset=['SMILES', 'label'])\n",
    "print(f\"After removing NaN values: {df.shape}\")\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\nLabel distribution (normalized):\")\n",
    "print(df['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bfc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaffold splitting function\n",
    "def scaffold_split(df, test_size=0.2, valid_size=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Split dataset based on molecular scaffolds to avoid data leakage\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate scaffolds for each molecule\n",
    "    scaffolds = defaultdict(list)\n",
    "    \n",
    "    for idx, smiles in enumerate(df['SMILES']):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                scaffold = MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=False)\n",
    "                scaffolds[scaffold].append(idx)\n",
    "            else:\n",
    "                # If molecule parsing fails, use SMILES as scaffold\n",
    "                scaffolds[smiles].append(idx)\n",
    "        except:\n",
    "            # If scaffold generation fails, use SMILES as scaffold\n",
    "            scaffolds[smiles].append(idx)\n",
    "    \n",
    "    # Sort scaffolds by size (largest first)\n",
    "    scaffold_sets = list(scaffolds.values())\n",
    "    scaffold_sets.sort(key=len, reverse=True)\n",
    "    \n",
    "    # Split scaffolds into train, valid, test\n",
    "    total_size = len(df)\n",
    "    test_target = int(total_size * test_size)\n",
    "    valid_target = int(total_size * valid_size)\n",
    "    \n",
    "    train_indices, valid_indices, test_indices = [], [], []\n",
    "    train_size, valid_size_current, test_size_current = 0, 0, 0\n",
    "    \n",
    "    for scaffold_set in scaffold_sets:\n",
    "        if test_size_current < test_target:\n",
    "            test_indices.extend(scaffold_set)\n",
    "            test_size_current += len(scaffold_set)\n",
    "        elif valid_size_current < valid_target:\n",
    "            valid_indices.extend(scaffold_set)\n",
    "            valid_size_current += len(scaffold_set)\n",
    "        else:\n",
    "            train_indices.extend(scaffold_set)\n",
    "            train_size += len(scaffold_set)\n",
    "    \n",
    "    train_df = df.iloc[train_indices].reset_index(drop=True)\n",
    "    valid_df = df.iloc[valid_indices].reset_index(drop=True)\n",
    "    test_df = df.iloc[test_indices].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting based on selected method\n",
    "if SPLITTING_METHOD == \"random\":\n",
    "    print(\"Using Random Splitting...\")\n",
    "    \n",
    "    # First split: train+valid vs test\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs valid\n",
    "    train_df, valid_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=VALID_SIZE,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {train_df.shape}\")\n",
    "    print(f\"Valid: {valid_df.shape}\")\n",
    "    print(f\"Test : {test_df.shape}\")\n",
    "    \n",
    "    # Check label distribution\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(\"Train:\\n\", train_df[\"label\"].value_counts(normalize=True))\n",
    "    print(\"Valid:\\n\", valid_df[\"label\"].value_counts(normalize=True))\n",
    "    print(\"Test:\\n\", test_df[\"label\"].value_counts(normalize=True))\n",
    "\n",
    "elif SPLITTING_METHOD == \"scaffold\":\n",
    "    print(\"Using Scaffold Splitting...\")\n",
    "    \n",
    "    train_df, valid_df, test_df = scaffold_split(df, TEST_SIZE, VALID_SIZE, SEED)\n",
    "    \n",
    "    print(f\"Train: {train_df.shape}\")\n",
    "    print(f\"Valid: {valid_df.shape}\")\n",
    "    print(f\"Test : {test_df.shape}\")\n",
    "    \n",
    "    # Check label distribution\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(\"Train:\\n\", train_df[\"label\"].value_counts(normalize=True))\n",
    "    print(\"Valid:\\n\", valid_df[\"label\"].value_counts(normalize=True))\n",
    "    print(\"Test:\\n\", test_df[\"label\"].value_counts(normalize=True))\n",
    "\n",
    "elif SPLITTING_METHOD == \"stratified_kfold\":\n",
    "    print(f\"Using Stratified K-Fold Cross-Validation (k={N_SPLITS})...\")\n",
    "    \n",
    "    # For K-fold, we'll use the entire dataset and split during CV\n",
    "    # Keep a separate test set for final evaluation\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=TEST_SIZE,\n",
    "        stratify=df[\"label\"],\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    \n",
    "    print(f\"Train+Valid: {train_val_df.shape}\")\n",
    "    print(f\"Test: {test_df.shape}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown splitting method: {SPLITTING_METHOD}\")\n",
    "\n",
    "\n",
    "# Add logging steps calculation\n",
    "if SPLITTING_METHOD != \"stratified_kfold\":\n",
    "    classification_args['logging_steps'] = max(1, len(train_df) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8876e2",
   "metadata": {},
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87360a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLITTING_METHOD in [\"random\", \"scaffold\"]:\n",
    "    print(f\"\\nTraining with {SPLITTING_METHOD} splitting...\")\n",
    "    \n",
    "    # Prepare data for simpletransformers (rename columns)\n",
    "    train_df_model = train_df.rename(columns={\"SMILES\": \"text\", \"label\": \"labels\"}).copy()\n",
    "    valid_df_model = valid_df.rename(columns={\"SMILES\": \"text\", \"label\": \"labels\"}).copy()\n",
    "    test_df_model = test_df.rename(columns={\"SMILES\": \"text\", \"label\": \"labels\"}).copy()\n",
    "    classification_args[\"output_dir\"] = f\"outputs/exp_2_bbbp_{SPLITTING_METHOD}/\"\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ClassificationModel('roberta', 'DeepChem/ChemBERTa-77M-MLM', args=classification_args)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    model.train_model(train_df_model, eval_df=valid_df_model)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    result, model_outputs, wrong_predictions = model.eval_model(test_df_model, acc=accuracy_score)\n",
    "    print(\"Test Results:\", result)\n",
    "\n",
    "elif SPLITTING_METHOD == \"stratified_kfold\":\n",
    "    print(f\"\\nTraining with Stratified K-Fold Cross-Validation (k={N_SPLITS})...\")\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    X = train_val_df[\"SMILES\"]\n",
    "    y = train_val_df[\"label\"]\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    acc_list = []\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        print(f\"\\n=== Fold {fold}/{N_SPLITS} ===\")\n",
    "\n",
    "        fold_train_df = train_val_df.iloc[train_idx].rename(columns={\"SMILES\": \"text\", \"label\": \"labels\"}).copy()\n",
    "        fold_val_df = train_val_df.iloc[val_idx].rename(columns={\"SMILES\": \"text\", \"label\": \"labels\"}).copy()\n",
    "\n",
    "        classification_args[\"logging_steps\"] = max(1, len(fold_train_df) // BATCH_SIZE)\n",
    "        fold_dir = os.path.join(MODEL_FOLDER, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        classification_args[\"best_model_dir\"] = fold_dir\n",
    "        classification_args[\"output_dir\"] = f\"outputs/exp_2_clintox_random/fold_{fold}\"\n",
    "\n",
    "        model = ClassificationModel(\"roberta\", \"DeepChem/ChemBERTa-77M-MLM\", args=classification_args, cuda_device=4, use_cuda=True)\n",
    "\n",
    "        print(f\"Training fold {fold}...\")\n",
    "        model.train_model(fold_train_df, eval_df=fold_val_df)\n",
    "\n",
    "        result, _, _ = model.eval_model(fold_val_df, acc=accuracy_score)\n",
    "        acc = result.get(\"acc\", 0)\n",
    "        print(f\"Fold {fold} Accuracy: {acc:.4f}\")\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        fold_results.append(result)\n",
    "\n",
    "    # K-Fold summary\n",
    "    print(\"\\nCross-validation summary:\")\n",
    "    for i, acc in enumerate(acc_list, start=1):\n",
    "        print(f\"Fold-{i}: {acc:.4f}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
